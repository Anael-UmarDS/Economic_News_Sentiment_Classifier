{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Economic News Sentiment Classifier"
      ],
      "metadata": {
        "id": "ZRhc-KLRpT0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Author\n",
        "\n",
        "Anael Umar\n",
        "\n",
        "**GitHub**: [Anael-UmarDS](https://github.com/Anael-UmarDS)\n",
        "\n",
        "---\n",
        "\n",
        "## Description\n",
        "\n",
        "This project pulls **real-time economic headlines** from [NewsAPI](https://newsapi.org) and automatically labels them using **VADER sentiment analysis**. It then trains a **logistic regression classifier** to categorize future headlines as **positive**, **neutral**, or **negative**. The final output includes evaluation metrics and trend **plots** to visualize **sentiment shifts** over time.\n",
        "\n",
        "---\n",
        "\n",
        "## Objectives\n",
        "\n",
        "- Applying machine learning to understand **live financial data**\n",
        "- Automate labeling of economic headlines using **natural language processing (NLP)** tools\n",
        "- **Visualize** economic sentiment trends over a strict period of time\n",
        "- Provide a **foundation** for further research in economic forecasting and AI-based sentiment analysis\n",
        "- **Applying machine learning** to understand the economy at a **macroeconomic level**\n",
        "\n",
        "---\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Data Fetching**  \n",
        "   Headlines are fetched using the NewsAPI for the keyword **economy**.\n",
        "\n",
        "2. **Auto-labeling with VADER**  \n",
        "   Each headline is scored using VADER sentiment analysis and categorized into:\n",
        "   - **positive**\n",
        "   - **neutral**\n",
        "   - **negative**\n",
        "\n",
        "3. **Training a Classifier**  \n",
        "   A logistic regression model is trained on the auto-labeled data using TF-IDF features.\n",
        "\n",
        "4. **Evaluation**  \n",
        "   The model is evaluated using accuracy, precision, recall, F1-score, and a confusion matrix.\n",
        "\n",
        "5. **Visualization**  \n",
        "   Plots are generated to track:\n",
        "   - Percentage of each sentiment by week\n",
        "   - Sentiment trends over time for economic headlines\n",
        "\n",
        "---\n",
        "\n",
        "## Example Use Cases\n",
        "\n",
        "- Predicting **market mood shifts** based on economic news sentiment\n",
        "- **Tracking** government or policy-related economic tone\n",
        "- Training ML pipelines with **auto-labeled** real-world data\n",
        "- Demonstration of **applied machine learning** and **NLP**\n",
        "\n",
        "---\n",
        "\n",
        "## Visual Outputs\n",
        "\n",
        "- **Confusion Matrix**\n",
        "- **Line Charts** of Sentiment Percentage per Week  \n",
        "- Real world **demo headlines** pulled from Google News for testing\n",
        "\n",
        "---\n",
        "## Limitations\n",
        "\n",
        "- **Headline Context**: Headlines lack full article context, which could influence sentiment accuracy\n",
        "- **Bias**: Certain media sources may exhibit political or economic bias that skews sentiment labeling\n",
        "- **Imbalanced Labels**: Economic headlines are often skewed toward neutral sentiment, which can affect model generalization\n",
        "- **Query Limit for Free Developer Plan**: The API free Developer plan only allows up to 100 requests per day and only articles up to a month old, therefore forcing training data to be short and visualization to plot weekly headline percentage over a total of 1 month\n",
        "---\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "1. Get your free NewsAPI key by going to https://newsapi.org or clicking the link above in **Description**\n",
        "2. Click **\"Get API Key\"** and register with your information\n",
        "3. Replace your key in the cell under **API Setup** with where it says **\"YOUR KEY HERE\"**\n",
        "4. Run the notebook in Jupyter Labs, Google Colab, or in a Python file"
      ],
      "metadata": {
        "id": "8UyoXmN4pQh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "D6tMqUL7CLvO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "1Fao-nrvOEXL"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "\n",
        "import requests                                                                                 # Allows for HTTP requests to be sent from Python\n",
        "import pandas as pd                                                                             # Used for data manipulation and analysis\n",
        "import numpy as np                                                                              # Used to store data in multi-dimensional arrays\n",
        "import matplotlib.pyplot as plt                                                                 # Used to illustrate data as plots or graphs\n",
        "import joblib                                                                                   # Used to run large computations and efficiently handle them\n",
        "from collections import Counter                                                                 # Used to iterate through hashable objects\n",
        "from typing import List                                                                         # Used to indicate the expected types of variables, function parameters, and return values\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer                                           # Give a sentiment intensity score to sentences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer                                     # Convert a collection of raw documents to a matrix of TF-IDF features\n",
        "from sklearn.linear_model import LogisticRegression                                             # Used for multi-class classification problems\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix             # Used for accuracy of a classification model, text report showing the main classification metrics for each class, computing a confusion matrix, respectively\n",
        "from sklearn.model_selection import train_test_split                                            # Split arrays or matrices into random train and test subsets\n",
        "import nltk                                                                                     # Used for working with Natural Language Processing (NLP)\n",
        "from textblob import TextBlob                                                                   # Used for processing textual data\n",
        "\n",
        "nltk.download(\"vader_lexicon\")                                                                  # Using NLTK Library to download VADER, a sentiment analysis tool"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Setup"
      ],
      "metadata": {
        "id": "2ca8-K-lCaze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEWSAPI_KEY = \"YOUR KEY HERE\"                                                                   # API Key\n",
        "NEWSAPI_ENDPOINT = \"https://newsapi.org/v2/everything\"                                          # Point of Contact for API\n",
        "RANDOM_STATE = 42                                                                               # Ensures Reproducibility\n",
        "LABELS = [\"positive\", \"neutral\", \"negative\"]                                                    # Labels for Classification"
      ],
      "metadata": {
        "id": "xQDpoZsBOKV1"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetching and Labeling Data"
      ],
      "metadata": {
        "id": "f-I2ofVlCTya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the API and retrieve the data\n",
        "# (headlines related to the economy and when they were published)\n",
        "# and store in Pandas DataFrame returning headlines column\n",
        "# Define parameters and return type using typing library\n",
        "\n",
        "def fetch_news_data(query=\"economy\", page_size=100) -> pd.DataFrame:\n",
        "\n",
        "    # Access API parameters/necessities\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"language\": \"en\",\n",
        "        \"pageSize\": page_size,\n",
        "        \"apiKey\": NEWSAPI_KEY\n",
        "    }\n",
        "\n",
        "    # Access HTTP server to access API\n",
        "    response = requests.get(NEWSAPI_ENDPOINT, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Handle Exceptions\n",
        "    if response.status_code != 200 or \"articles\" not in data:\n",
        "        raise ValueError(\"Failed to fetch news data from NewsAPI\")\n",
        "\n",
        "    # Create headline column for Pandas DataFrame\n",
        "    headlines = [\n",
        "      {\n",
        "          \"headline\": article[\"title\"],\n",
        "          \"publishedAt\": article.get(\"publishedAt\")\n",
        "      }\n",
        "      for article in data[\"articles\"]\n",
        "      if article[\"title\"]\n",
        "    ]\n",
        "\n",
        "    return pd.DataFrame(headlines)"
      ],
      "metadata": {
        "id": "AYUZsWn3ON76"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label returned Pandas DataFrame of fetch_news_data with labels\n",
        "# based on sentiment value provided by NLTK VADER\n",
        "# and return it as a new Pandas DataFrame\n",
        "# Define parameters and return type using typing library\n",
        "\n",
        "def auto_label_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    # Create labeled_rows column for Pandas DataFrame\n",
        "    labeled_rows = []\n",
        "    for i, row in df.iterrows():\n",
        "        headline = row[\"headline\"]\n",
        "        polarity = TextBlob(headline).sentiment.polarity\n",
        "\n",
        "        # Auto-label based on polarity\n",
        "        if polarity > 0.1:\n",
        "            label = \"positive\"\n",
        "        elif polarity < -0.1:\n",
        "            label = \"negative\"\n",
        "        else:\n",
        "            label = \"neutral\"\n",
        "\n",
        "        # Extract date from publishedAt\n",
        "        date_str = str(row.get(\"publishedAt\", \"\"))[:10]\n",
        "        try:\n",
        "            date_obj = pd.to_datetime(date_str)\n",
        "            week = date_obj.to_period(\"W\").start_time.date()\n",
        "            month = date_obj.strftime(\"%Y-%m\")\n",
        "        except Exception:\n",
        "            week = pd.NaT\n",
        "            month = None\n",
        "\n",
        "        labeled_rows.append({\n",
        "            \"headline\": headline,\n",
        "            \"sentiment\": label,\n",
        "            \"week\": week,\n",
        "            \"date\": month\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(labeled_rows)"
      ],
      "metadata": {
        "id": "oDPTbytEOT--"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning and Preparing Data for Training and Evaluation"
      ],
      "metadata": {
        "id": "A2brCYzHCntH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean data to lowercase and no blanks\n",
        "# Helper function\n",
        "\n",
        "def basic_text_clean(text: str) -> str:\n",
        "    return text.lower().strip()"
      ],
      "metadata": {
        "id": "I93hX8CpOXKB"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Pandas DataFrame using helper function\n",
        "\n",
        "def prepare_features(df: pd.DataFrame):\n",
        "    X_raw = df[\"headline\"].apply(basic_text_clean)\n",
        "    y = df[\"sentiment\"]\n",
        "    return X_raw, y"
      ],
      "metadata": {
        "id": "XiSAAFT5OaEW"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Evaluate Data"
      ],
      "metadata": {
        "id": "MJziLbekCtJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function to train model to classify headlines\n",
        "# TF-IDF (Term Frequency - Inverse Document Frequency)\n",
        "# TF: Term Frequency: number of times the term appears in a document compared to the total number of words in the document\n",
        "# IDF: Inverse Document Frequency: ratio of total number of documents to number of documents containing the term\n",
        "# Uses TF-IDF Sentiment Weightage System: TF-IDF = TF * IDF\n",
        "# Low TF-IDF indicates high frequency of a term\n",
        "# Logistic regression: statistical analysis method used to predict multinomial outcome\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    vectoriser = TfidfVectorizer(max_features=6000, ngram_range=(1, 2), stop_words=\"english\")                                   # Convert text into numerical features\n",
        "    X_train_vec = vectoriser.fit_transform(X_train)                                                                             # Fit the vectorizer to the training text data (X_train) and then transform it (evaluating how important word is to document)\n",
        "    model = LogisticRegression(max_iter=2000, n_jobs=-1, class_weight=\"balanced\")                                               # Initialize a Logistic Regression model for classification\n",
        "    model.fit(X_train_vec, y_train)                                                                                             # Train the Logistic Regression model using the vectorized training features (X_train_vec) and their corresponding labels (y_train)\n",
        "    return model, vectoriser                                                                                                    # Return trained model and numerical features"
      ],
      "metadata": {
        "id": "6IkZ4WU8OcmD"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicts label for each headline based on sentiment value\n",
        "\n",
        "def predict(model, vectoriser, headlines: List[str]):\n",
        "    cleaned = [basic_text_clean(h) for h in headlines]\n",
        "    return model.predict(vectoriser.transform(cleaned))"
      ],
      "metadata": {
        "id": "sVeExQojOmml"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of the model by calculating the accuracy of predictions and calling plot_confusion_matrix\n",
        "# Calculates precision, recall, and trade-off rate of precision and recall (f1-score)\n",
        "\n",
        "def evaluate_model(model, vectoriser, X_test, y_test):\n",
        "    X_test_vec = vectoriser.transform(X_test)\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "    print(\"\\nEvaluation:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\\n\")\n",
        "    print(classification_report(y_test, y_pred, digits=3))\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=LABELS)\n",
        "    plot_confusion_matrix(cm)"
      ],
      "metadata": {
        "id": "0UyJwBHMOfoS"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Data"
      ],
      "metadata": {
        "id": "CpsBoAUWCznR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots confusion matrix that compares true labels vs assigned labels\n",
        "# And saves as an image called \"confusion_matrix.png\"\n",
        "\n",
        "def plot_confusion_matrix(cm: np.ndarray):\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set(xticks=np.arange(len(LABELS)),\n",
        "           yticks=np.arange(len(LABELS)),\n",
        "           xticklabels=LABELS,\n",
        "           yticklabels=LABELS,\n",
        "           ylabel=\"True label\",\n",
        "           xlabel=\"Predicted label\",\n",
        "           title=\"Confusion Matrix\")\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], \"d\"), ha=\"center\", va=\"center\", color=\"black\")\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(\"confusion_matrix.png\", dpi=300)\n",
        "    plt.close()\n",
        "    print(\"Confusion matrix saved → confusion_matrix.png\")"
      ],
      "metadata": {
        "id": "_kZh6WArOiLm"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots each sentiment percentage of total headlines per week\n",
        "# As three separate line trends where the x-axis is each week and the y-axis is the percentage of each sentiment\n",
        "\n",
        "def plot_sentiment_percentage_trend(labeled_df: pd.DataFrame):\n",
        "\n",
        "    # Ensure 'week' is datetime\n",
        "    labeled_df[\"week\"] = pd.to_datetime(labeled_df[\"week\"])\n",
        "\n",
        "    # Count sentiment per week\n",
        "    weekly_counts = labeled_df.groupby([\"week\", \"sentiment\"]).size().unstack(fill_value=0)\n",
        "\n",
        "    # Full weekly range from start to end of dataset\n",
        "    full_weeks = pd.date_range(\n",
        "        start=labeled_df[\"week\"].min(),\n",
        "        end=labeled_df[\"week\"].max(),\n",
        "        freq=\"W-MON\"\n",
        "    )\n",
        "\n",
        "    # Reindex to include all weeks\n",
        "    weekly_counts = weekly_counts.reindex(full_weeks, fill_value=0)\n",
        "    weekly_counts.index.name = \"week\"\n",
        "\n",
        "    # Calculate total per week and derive percentages\n",
        "    weekly_totals = weekly_counts.sum(axis=1)\n",
        "    sentiment_percentages = weekly_counts.divide(weekly_totals, axis=0).fillna(0) * 100\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for sentiment in [\"positive\", \"neutral\", \"negative\"]:\n",
        "        if sentiment in sentiment_percentages.columns:\n",
        "            plt.plot(sentiment_percentages.index, sentiment_percentages[sentiment], label=sentiment, marker=\"o\")\n",
        "\n",
        "    plt.title(\"Weekly Percentage of Economic Headlines by Sentiment\")\n",
        "    plt.xlabel(\"Week\")\n",
        "    plt.ylabel(\"Sentiment Share (%)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"line_trend.png\", dpi=300)\n",
        "    plt.close()\n",
        "    print(\"Line trend saved → line_trend.png\")\n",
        "    #plt.show()\n"
      ],
      "metadata": {
        "id": "umTtEFOc9uaR"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Efficiently save the models using joblib\n",
        "\n",
        "def save_artifacts(model, vectoriser):\n",
        "    joblib.dump(model, \"sentiment_model.joblib\")\n",
        "    joblib.dump(vectoriser, \"tfidf_vectorizer.joblib\")\n",
        "    print(\"Model artifacts saved.\")"
      ],
      "metadata": {
        "id": "6R_cSh_rOkYv"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function"
      ],
      "metadata": {
        "id": "35T_65gKC8AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Running Function\n",
        "def main():\n",
        "\n",
        "    # Number of queries to fetch\n",
        "    n = 100\n",
        "\n",
        "    # Fetch query from API (see function documentation comments on how)\n",
        "    news_df = fetch_news_data(page_size=n)\n",
        "\n",
        "    # Label query from API (see function documentation comments on how)\n",
        "    labeled_df = auto_label_data(news_df)\n",
        "\n",
        "    # Print how many headlines are of each sentiment based on VADER sentiment analysis\n",
        "    print(\"\\nClass distribution:\", dict(Counter(labeled_df[\"sentiment\"])))\n",
        "\n",
        "    # Clean data and prepare for training\n",
        "    # Implement stratified random sampling\n",
        "    # Stratified random sampling splits data into subsets\n",
        "    # and randomly chooses each from there until None are left\n",
        "    X, y = prepare_features(labeled_df)\n",
        "    min_class_size = min(Counter(y).values())\n",
        "    can_stratify = min_class_size >= 2\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=y if can_stratify else None, random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # Train model (see function documentation comments on how)\n",
        "    model, vectoriser = train_model(X_train, y_train)\n",
        "\n",
        "    # Evaluate model (see function documentation comments on how)\n",
        "    evaluate_model(model, vectoriser, X_test, y_test)\n",
        "\n",
        "    # Save models using save_artifact (see function documentation comments on how)\n",
        "    save_artifacts(model, vectoriser)\n",
        "\n",
        "    # List of real demo headlines pulled at random from Google News\n",
        "    demo_headlines = [\n",
        "      \"The S&P 500 Is Projected to Rally More Than Expected\",\n",
        "      \"What Division Inside the Fed Means for Future Interest-Rate Cuts\",\n",
        "      \"Citing trade wars, the World Bank sharply downgrades global economic growth forecast to 2.3%\",\n",
        "      \"Trump Threatens 30% Tariffs on EU, Mexico\",\n",
        "      \"Faisal Islam: We are heading for significant tax rises\",\n",
        "      \"Reeves disappointed after economy unexpectedly shrinks\",\n",
        "    ]\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nDemo predictions:\")\n",
        "    for h, p in zip(demo_headlines, predict(model, vectoriser, demo_headlines)):\n",
        "        print(f\"[{p}] {h}\")"
      ],
      "metadata": {
        "id": "BHTp2Loo-P2w"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Main"
      ],
      "metadata": {
        "id": "m8Mr8JCwC_UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run main()\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kQJ39GhW61fN"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "- **Sentiment classification** on news headlines is feasible using lightweight models\n",
        "- **Weekly sentiment trends** can uncover **mood shifts** in the economy\n",
        "- Even rule-based systems like **VADER** can support training useful **supervised models**\n",
        "- The classifier achieved **reasonable performance** in identifying headline sentiment automatically\n",
        "\n",
        "---\n",
        "\n",
        "## Implications\n",
        "\n",
        "###Financial Markets\n",
        "\n",
        "- **Investor Sentiment Monitoring**: Investigate market mood in real-time\n",
        "- **Event-Driven Trading**: Identify spikes in optimism or fear to adjust strategy\n",
        "\n",
        "###Public Policy\n",
        "\n",
        "- **Macroeconomic Tracking**: Understand how the media presents economic news\n",
        "- **Crisis Detection**: Spot growing negative trends in headlines\n",
        "\n",
        "###ML Development\n",
        "\n",
        "- **Semi-Supervised Learning**: Demonstrates how to bootstrap datasets from unlabeled text\n",
        "- **Fine-Tuning Opportunity**: Base model could be improved using deep learning\n",
        "\n",
        "---\n",
        "\n",
        "## Machine Learning Concepts\n",
        "\n",
        "- API\n",
        "- Natural Language Processing (NLP)\n",
        "- Feature Engineering\n",
        "- Model Training & Evaluation\n",
        "- Data Visualization"
      ],
      "metadata": {
        "id": "WQOwAsScAK5X"
      }
    }
  ]
}